import os
import sys
import numpy as np
import pandas as pd
import itertools
try:
    from src.utils.systems_collection import systems_collection
    from src.utils.proged_utils import get_fit_settings, get_grammar_type
except ImportError:
    from utils.systems_collection import systems_collection
    from utils.proged_utils import get_fit_settings, get_grammar_type
import ProGED as pg


def configure_proged(systems, path_data, fname_data, iinit, eq_sym, snrs=[None, 30, 13], observability="full", observed_vars=["x", "y"],
               path_out=".\\results\\check_proged\\", path_structures=".\\results\\proged\\structures\\",
               fname_structures="structures.csv"):

    """
    Run ProGED on the data generated by the function generate_data().

    Arguments:
        - systems             (list)                    list of <System> objects (see file system.py)
        - path_data           (string)                  path to the folder where the data is saved
        - fname_data          (string)                  name of the csv file with the data
        - iinit               (int)                     index of the initial condition to be used
        - eq_sym              (string)                  symbol of the equation to be identified
        - snrs                (list)                    list of desired Signal-to-Noise Ratios (in dB). If None (default),
                                                        no noise will be added, otherwise float or int values should be provided.
        - observability       (string)                  either 'full' or 'partial' (default = 'full')
        - path_out            (string)                  path to the folder where the results should be saved
        - use_default_library (bool)                    if True, the default library of functions is used for SINDy
                                                         (see pysindy documentation for details)

    Returns:
        Nothing. Results are saved in the folder specified by path_out.

    Example:
        Example run is shown at the end of the script.

    """

    for system_name in systems:
        for snr in snrs:

            print(f"{method} | {system_name} | snr: {snr} | init: {iinit} | batch: {ib} | eq: {eq_sym}")

            # get structures
            systemBox = pg.ModelBox()
            systemBox.load(f"{path_structures}{fname_structures}{os.sep}{fname_structures}_b{ib}.pg")

            # get data (and organize it for proged)
            data_orig = pd.read_csv(f"{path_data}{system_name}{os.sep}{fname_data}".format(system_name, snr, iinit))

            # setings based on observability
            if observability == "full":
                # include also derivatives of the state var equation (the target) that will be identified
                data = data_orig[['t'] + systems[system_name].state_vars + ['d' + eq_sym]]
                task_type = "algebraic"
                for model in range(len(systemBox)):
                    systemBox[model].lhs_vars = ['d' + eq_sym]
                    systemBox[model].observed_vars = observed_vars
            else:
                # take all observed ones
                data = data_orig[['t'] + observed_vars]
                task_type = "differential"
                for model in range(len(systemBox)):
                    systemBox[model].lhs_vars = systems[system_name].state_vars
                    systemBox[model].observed_vars = observed_vars

            # settings for parameter estimation
            estimation_settings = get_fit_settings(obs=observed_vars, task_type=task_type)
            estimation_settings["parameter_estimation"]["param_bounds"] = (tuple(systems[system_name].param_bounds),)

            # -------------------------------------------------------------
            # ----------- RUN PROGED PARAMETER ESTIMATION------------------
            # -------------------------------------------------------------
            systemBox_fitted = pg.fit_models(systemBox, data=data, settings=estimation_settings)

            # SAVE the fitted models and the settings file
            os.makedirs( f"{path_out}{system_name}{os.sep}", exist_ok=True)
            obs_txt = "".join(observed_vars)
            out_filename = f"{method}_{exp_type}_{exp_version}_{system_name}_{data_type}_{data_size}_" \
                           f"snr{snr}_init{iinit}_obs{obs_txt}_b{ib}_{eq_sym}_fitted.pg"
            systemBox_fitted.dump(f"{path_out}{system_name}{os.sep}{out_filename}")


def get_configuration(iinput, systems_collection, data_sizes, snrs, n_data, n_batches, observability="full", unsuccessful_configs_path=None):

    if unsuccessful_configs_path is None:
        # observability scenarios
        obss = [['x'], ['y'], ['x', 'y']] if observability == "partial" else [['x', 'y']]
        obss_lorenz = [['x', 'y'], ['x', 'z'], ['y', 'z'], ['x', 'y', 'z']] if observability == "partial" else [['x', 'y', 'z']]
        eq_sym = 'xy' if observability == "partial" else ['x', 'y'] # if partial, identify all equations in the model together
        eq_sym_lorenz = 'xyz' if observability == "partial" else ['x', 'y', 'z']

        # check if lorenz is in the keys of the systems_collection and adjust the combinations of state vars accordingly
        if 'lorenz' in systems_collection.keys():
            # remove lorenz from system_collection
            new_systems_collection = systems_collection.copy()
            lorenz = {'lorenz': new_systems_collection.pop('lorenz')}
            combinations_other = list(itertools.product(new_systems_collection, data_sizes, snrs, obss,
                                                        np.arange(n_data), np.arange(n_batches), eq_sym))
            combinations_lorenz = list(itertools.product(lorenz, data_sizes, snrs, obss_lorenz,
                                                         np.arange(n_data), np.arange(n_batches), eq_sym_lorenz))
            combinations = combinations_other + combinations_lorenz
        else:
            combinations = list(itertools.product(systems_collection, data_sizes, snrs, obss,
                                                  np.arange(n_data), np.arange(n_batches), eq_sym))
        return combinations[iinput]

    else:
        df = pd.read_csv(unsuccessful_configs_path, sep='\t')
        system_name = df.loc[iinput, 'system']
        data_size = df.loc[iinput, 'data_size']
        snr = int(df.loc[iinput, 'snr']) if not np.isnan(df.loc[iinput, 'snr']) else None
        obs = df.loc[iinput, 'obs']
        iinit = df.loc[iinput, 'iinit']
        ib = df.loc[iinput, 'ib']
        eq_sym = df.loc[iinput, 'eq']
        return system_name, data_size, snr, obs, iinit, ib, eq_sym


##
if __name__ == '__main__':

    # This code is only for testing locally. It should be run on HPC cluster, where the jobs can be run in parallel.
    # Each job should be run with different iinput, which can be specified in the job script (by slurm array).
    # The code here needs to be changed so that iinput = 0 is commented out. See the job script for details.

    # -------------------------------------------------------------
    # ------------------------- SETTINGS -------------------------
    # -------------------------------------------------------------

    # iinput = int(sys.argv[1])     # Uncomment this out if running on cluster.
    iinput = 0                      # Comment this out if running on cluster.

    # experiment settings
    # root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    # root_dir = "."  # for HPC cluster (uncomment if running on HPC cluster)
    root_dir = "D:\\Experiments\\symreg_methods_comparison"  # (comment out if running on HPC cluster)
    sys.path.append(root_dir)

    method = "proged"
    exp_version = "e1"  # "e1" (constrained model search space) or "e2" (unconstrained model search space)
    observability = "partial"  # "full" or "partial"
    exp_type = f"sysident_{observability}"
    only_redo_unsuccessful = False    # if True, only the models that were not successfully identified in the previous run will be identified

    # data settings
    data_type = "train"
    data_sizes = ["small", "large"]
    snrs = [None, 30, 13]   # signal-to-noise ratio
    n_data = 4  # number of different data sets (have different initial values)

    # model space (structures) settings
    n_batches = 100
    n_samples = 2500
    use_universal_library = True if exp_version == "e2" else False  # if True, the universal library is used, otherwise the library is specific for each system

    # take only vdp system if observability is partial
    if observability == "partial":
        systems_collection = {k: v for k, v in systems_collection.items() if k == "vdp"}

    # On the cluster, some jobs can fail for various reasons. After the results are loaded back to local computer,
    # the script "proged_find_nonsuccessful_fittings.py" can be used to create a csv file with the configurations
    # of the models that were not successfully identified. This file can be used to rerun only the models that were not
    # successfully identified.
    if only_redo_unsuccessful:
        unsuccessful_configs_path = f"{root_dir}{os.sep}analysis{os.sep}{exp_type}{os.sep}unsuccessfully_ran_models_" \
                                    f"{method}_{exp_version}_{exp_type}.csv"
    else:
        unsuccessful_configs_path = None

    # get current configuration (each parallel job will get different configuration)
    system_name, data_size, snr, observed_vars, iinit, ib, eq_sym = get_configuration(iinput, systems_collection,
                                                                data_sizes, snrs, n_data, n_batches,
                                                                observability=observability,
                                                                unsuccessful_configs_path=unsuccessful_configs_path)

    # more settings
    grammar_type = get_grammar_type(system_name, universal=use_universal_library)
    systems = {system_name: systems_collection[system_name]}
    time_end = 20 if data_size == "large" else 10
    time_step = 0.01 if data_size == "large" else 0.1

    # paths
    path_data = f"{root_dir}{os.sep}data{os.sep}{data_type}{os.sep}{data_size}{os.sep}"
    fname_data = f"data_{{}}_len{time_end}_rate{str(time_step).replace('.', '')}_snr{{}}_init{{}}.csv"
    path_structures = f"{root_dir}{os.sep}results{os.sep}{exp_type}{os.sep}{method}{os.sep}structures{os.sep}"
    fname_structures = f"structs_{grammar_type}_nsamp{n_samples}_nbatch{n_batches}"
    path_out = f"{root_dir}{os.sep}results{os.sep}{exp_type}{os.sep}{method}{os.sep}{exp_version}{os.sep}"

    # -------------------------------------------------------------
    # ------------------------- CONFIGURE PROGED  -----------------
    # -------------------------------------------------------------

    # main function that calls proged
    configure_proged(systems=systems,
                    path_data=path_data,
                    fname_data=fname_data,
                    iinit=iinit,
                    eq_sym=eq_sym,
                    snrs=[snr],
                    observability=observability,
                    observed_vars=observed_vars,
                    path_out=path_out,
                    path_structures=path_structures,
                    fname_structures=fname_structures)

##

